---
title: "Data Analysis and Modeling in R"
subtitle: "AWARE AI - 01/17/24"
author: "Dustin Haraden, PhD"
format: 
  revealjs:
    multiplex: true
    scrollable: true
    slide-number: true
    incremental: false
    touch: true
    code-overflow: wrap
    highlightLines: true
    theme: dark
execute: 
  echo: true
editor: visual
editor_options: 
  chunk_output_type: console
---

```{r, include = F}
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
```

```{r, echo = F, results='hide',warning=FALSE,message=FALSE}
options(scipen = 999)

library(knitr)
# function to display only part of the output
hook_output <- knit_hooks$get("output")
knit_hooks$set(output = function(x, options) {
  lines <- options$output.lines
  if (is.null(lines)) {
    return(hook_output(x, options))  # pass to default hook
  }
  x <- unlist(strsplit(x, "\n"))
  more <- "..."
  if (length(lines)==1) {        # first n lines
    if (length(x) > lines) {
      # truncate the output, but add ....
      x <- c(head(x, lines), more)
    }
  } else {
    x <- c(more, x[lines], more)
  }
  # paste these lines together
  x <- paste(c(x, ""), collapse = "\n")
  hook_output(x, options)
})

knitr::opts_chunk$set(message = FALSE, 
                      warning = FALSE) # suppress messages
```

## About Me

::: {style="font-size: 30px"}
**Dustin Haraden, PhD**

Clinical/Community Psychology

Research Focus: Developmental Psychopathology

::: nonincremental
-   Examining Risk factors for depression and anxiety in children and adolescents
-   Emphasis on circadian rhythms and sleep processes as well as pubertal development
-   Investigating methodology as it impacts conclusions that can be made
-   Quantitatively Oriented
:::

Want to expand into integrating AI and LLM's into clinical practice to increase efficacy of treatment and adherence to evidence based practices (e.g., clear case conceptualization and treatment planning)
:::

------------------------------------------------------------------------

## Overview

-   Model Based Inquiries

-   "Hit by a bus" workflow

```{r, results = 'hide', message = F, warning = F}
options(scipen=999)
library(car) #recode conflict with tidyverse
library(tidyverse)
library(rio)
library(broom)
library(psych)
library(easystats)
library(sjPlot)
library(janitor)
```

------------------------------------------------------------------------

### Data for Today

#Need to update with data that I can use for demo purposes

```{r}

student_perf <- read_csv("https://raw.githubusercontent.com/dharaden/psyc640/main/data/Multiple_reg/Student_Performance.csv") %>% 
  clean_names()
```

# Model Based Inquiries

![](https://quotefancy.com/media/wallpaper/3840x2160/1503109-George-E-P-Box-Quote-All-models-are-wrong-but-some-are-useful.jpg){fig-align="center"}

------------------------------------------------------------------------

## Endogeneity

There exists a relationship between your predictors and unexplained variation (i.e., error) in your outcome

Some sources of endogeneity:

::: {.incremental style="font-size: 30px"}
-   Omitted Variables
    -   Failure to include relevant variables into the model
-   Measurement Error
    -   Constructs are measured ineffectively
-   Sample Selection
    -   The sample that was assessed was not done randomly or fails to represent the population of interest
:::

------------------------------------------------------------------------

## Endogeneity - Model Impact

Endogeneity can impact a model in a number of ways including:

-   Bias in parameter estimates

-   Inflation of standard errors

-   Spurious results

-   Sensitivity to model changes

    ------------------------------------------------------------------------

### Example - Single Predictor

```{r}
fit1 <- lm(performance_index ~ hours_studied, 
           data = student_perf)
```

------------------------------------------------------------------------

### Example - Coefficient interpretation

[sjPlot](https://strengejacke.github.io/sjPlot/articles/tab_model_estimates.html)

```{r}
sjPlot::tab_model(fit1)
```

------------------------------------------------------------------------

### Example - Visualizing

How do we visualize this?

------------------------------------------------------------------------

### Example - Visualizing

How do we visualize this?

```{r}
car::avPlots(fit1)
```

------------------------------------------------------------------------

## Holding Constant??? Wut

```{r, output.lines = 10:15}
summary(fit1)
```

-   The average amount of sleep decreases by 0.08 hours for every 1 year older the youth his **holding the number of hours playing video games and the number of hours on social media constant.**

-   The average amount of sleep decreases by 0.004 hours for every 1 hour of social media use **holding age and hours of video game usage constant.**

What does this mean?? Also can be called "controlling for" or "taking into account" the other variables

Language comes from experimental research in which they can keep one condition unchanged while manipulating the other

------------------------------------------------------------------------

## Holding Constant - "Controlling for"

![](/images/control.gif){fig-align="center"}

Taken from [\@nickchk](https://twitter.com/nickchk)

------------------------------------------------------------------------

## Creating the Model

There can be many different ways in which we create and compare models with multiple predictors

::: incremental
-   **Simultaneous**: Enter all variables in the model at once
    -   Usually the most conservative and defensible approach (unless there is theory to support a hierarchical approach)
-   **Hierarchically**: Building a sequence of models where a single variable is included/excluded at each step
    -   This is **hierarchical/stepwise regression.** Different from HLM (Hierarchical Linear Modeling)
:::

------------------------------------------------------------------------

## Model Selection ([LSR 15.10](https://learningstatisticswithr.com/book/regression.html#modelselreg))

How can we tell if one model is "better" than the other (it explains more variance in outcome)?

-   Each predictor (or set of predictors) is investigated as to what it adds to the model when it is entered
-   The order of the variables depends on an *a priori* hypothesis

The concept is to ask how much variance is unexplained by our model. The leftovers can be compared to an alternate model to see if the new variable adds anything or if we should focus on **parsimony**

------------------------------------------------------------------------

### Model Comparison Example

```{r, , output.lines = 1:7}
#m.2 <- lm(Sleep_Hours_Non_Schoolnight ~ Ageyears + video_game_hrs, 
#           data = student_perf)
#m.3 <- lm(Sleep_Hours_Non_Schoolnight ~ Ageyears + #video_game_hrs + social_med_hrs, 
#           data = school)
#anova(m.2, m.3)
```

```{r}
#r2(m.2)
#r2(m.3)
```

------------------------------------------------------------------------

### Model Comparison - sjPlot

```{r}
#tab_model(m.2, m.3)
```

# Model Diagnostics

------------------------------------------------------------------------

## Checking the model ([LSR 15.9](https://learningstatisticswithr.com/book/regression.html#regressiondiagnostics))

Whenever we are looking at the regression model diagnostics, we are often considering the residual values. The types of residuals we can look at are:

::: incremental
-   Ordinary Residuals - Raw

-   Standardized Residuals

-   Studentized Residuals - Takes into account what the SD *would* have been with the removal of the $i$th observation
:::

------------------------------------------------------------------------

### Model Checks - Outlier

We tend to look for various things that may impact our results through the lens of residuals

**1) Outliers** - variables with high Studentized Residuals

![](/images/outlier.PNG){fig-align="center"}

------------------------------------------------------------------------

### Model Checks - Leverage

We tend to look for various things that may impact our results through the lens of residuals

**2) Leverage** - variable is different in all ways from others (not just residuals)

![](/images/leverage.PNG){fig-align="center"}

------------------------------------------------------------------------

### Model Checks - Influence

We tend to look for various things that may impact our results through the lens of residuals

**3) Influence** - outlier with high leverage (*Cook's Distance*)

![](/images/influence.PNG){fig-align="center"}

------------------------------------------------------------------------

### Model Checks - Plots

```{r}
#Cook's Distance
#plot(m.3, which = 4)
```

------------------------------------------------------------------------

### Model Checks - Plots

```{r}
#Leverage
#plot(m.3, which = 5)
```

------------------------------------------------------------------------

## Checking Collinearity

We need to check to see if our predictor variables are too highly correlated with each other

To do so we use ***variance inflation factors (VIF)***

-   There will be a VIF that is associated with each predictor in the model
-   Interpreting VIF ([link](https://www.statisticshowto.com/variance-inflation-factor/)) - Starts at 1 and goes up
    -   1 = Not Correlated
    -   1-5 = Moderately Correlated
    -   5+ = Highly Correlated

```{r}
#car::vif(m.3)
```

------------------------------------------------------------------------

## `easystats` making stats easy

Can check the model with a simple function

```{r}
#check_model(m.3)
```

# Rounding up Multiple Regression

It is a powerful and complex tool

# Next time...

-   More R fun
-   Group work

# Stop...Group Time

![](/images/mchammer.gif){fig-align="center"}
